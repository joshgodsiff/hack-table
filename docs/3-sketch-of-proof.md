# 3. Sketch of Proof

To demonstrate proper function of our system, we need to prove that most operations take log n + c time for some small constant c, and that a (key,value) lookup returns a key stored in the system with overwhelming probability.

We start with some definitions. For a k-bucket covering the distance range [2ᵢ, 2ᵢ⁺¹), define the index of the bucket to be i. Define the depth, h, of a node to be 160 - i, where i is the smallest index of a non-empty bucket. Define node y's bucket height in node x to be the index of the bucket into which x would insert y minus the index of x's least significant empty bucket. Because node IDs are randomly chosen, it follows that highly non-uniform distributions are unlikely. Thus with overwhelming probability the height of a any given node will be within a constant of log n for a system with n nodes. Moreover, the bucket height of the closest node to an ID in the kth-closest node will likely be within a constant of log k.

Our next step will be to assume the invariant that every k-bucket of every node contains at least one contact if a node exists in the appropriate range. Given this assumption, we show that the node lookup procedure is correct and takes logarithmic time. Suppose the closest node to the target ID has depth h. If none of this node's h most significant k-buckets is empty, the lookup procedure will find a node half as close (or rather whose distance is one bit shorter) in each step, and thus turn up the node in h - log k steps. If one of the node's k-buckets is empty, it could be the case that the target node resides in the range of the empty bucket. In this case, the final steps will not decrease the distance by half. However, the search will proceed exactly as though the bit in the key corresponding to the empty bucket had been flipped. Thus, the lookup algorithm will always return the closest node in h - log k steps. Moreover, once the closest node is found, the concurrency switches from α to k. The number of steps to find the remaining k - 1 closest nodes can be no more than the bucket height of the closest node in the kth-closest node, which is unlikely to be more than a constant plus log k.

To prove the correctness of the invariant, first consider the effects of bucket refreshing if the invariant holds. After being refreshed, a bucket will either contain k valid nodes or else contain every node in its range if fewer than k exist. (This follows from the correctness of the node lookup procedure.) New nodes that join will also be inserted into any buckets that are not full. Thus, the only way to violate the invariant is for there to exist k + 1 or more nodes in the range of a particular bucket, and for the k actually contained in the bucket all to fail with no intervening lookups or refreshes. However, k was precisely chosen for the probability of simultaneous failure within an hour (the maximum refresh time) to be small.

In practice, the probability of failure is much smaller than the probability of k nodes leaving within an hour, as every incoming or outgoing request updates nodes' buckets. This results from the symmetry of the XOR metric, because the IDs of the nodes with which a given node communicates during an incoming or outgoing request are distributed exactly compatibly with the node's bucket ranges.

Moreover, even if the invariant does fail for a single bucket in a single node, this will only affect running time (by adding a hop to some lookups), not correctness of node lookups. For a lookup to fail, k nodes on a lookup path must each lose k nodes in the same bucket with no intervening lookups or refreshes. If the different nodes' buckets have no overlap, this happens with probability 2⁻ᵏ². Otherwise, nodes appearing in multiple other nodes' buckets will likely have longer uptime and thus lower probability of failure.

Now we consider a (key,value) pair's recovery. When a (key,value) pair is published, it is populated at the k nodes closest to the key. It is also re-published every hour. Since even new nodes (the least reliable) have probability 1/2 of lasting one hour, after one hour the (key,value) pair will still be present on one of the k nodes closest to the key with probability 1 - 2⁻ᵏ. This property is not violated by the insertion of new nodes that are close to the key, because as soon as such nodes are inserted, they contact their closest nodes in order to fill their buckets and thereby receive any nearby (key,value) pairs they should store. Of course, if the k closest nodes to a key fail and the (key,value) pair has not been cached elsewhere, Kademlia will fail to store the pair and therefore lose the key.